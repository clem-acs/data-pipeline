# Proposal: Converting t2C_event_v0.py from HDF5 to Zarr

## Overview

This proposal outlines the changes required to convert the EventTransform (t2C_event_v0.py) from using HDF5 to Zarr format, aligning with the approach used in WindowTransform (t2A_window_v0.py). The goal is to provide a clean, efficient implementation with minimal code changes.

## Current Implementation

The EventTransform currently:
1. Downloads curated H5 files from S3
2. Extracts and processes event data into dictionaries
3. Converts these dictionaries to structured numpy arrays
4. Saves arrays to HDF5 using h5py with complex grouping
5. Uploads the HDF5 file to S3

## Key Principles for the Conversion

1. **Single Zarr Store**: Create a unified xarray Dataset with a single zarr store, not multiple datastores with separate paths
2. **Preserve Data Relationships**: Maintain the relationships between tasks, elements, and segments
3. **Dimension-Based Organization**: Utilize xarray's dimension-based model rather than HDF5's group-based approach
4. **Leveraging BaseTransform**: Fully utilize existing `save_dataset_to_s3_zarr()` method
5. **Memory Efficiency**: Handle large datasets with appropriate chunking strategies

## Implementation Changes

### 1. Process Flow Changes

The workflow changes from:
```
HDF5: Extract → Process → Save to HDF5 file → Upload file
```

To:
```
Zarr:  Extract → Process → Convert to xarray → Save directly to S3 zarr
```

### 2. Core Function Changes

#### A. Main Process Session Method

```python
def process_session(self, session: Session) -> Dict:
    """Process a single session."""
    # ... [existing file discovery and download code] ...

    try:
        # Extract and process events (unchanged)
        with h5py.File(local_source, 'r') as source_h5:
            events = self._extract_raw_events(source_h5)
            processed_data = self._process_event_data(events, session_id, source_h5)

        # Create xarray dataset from processed data
        dataset = self._create_xarray_dataset(processed_data)
        
        # Define the destination zarr key
        zarr_key = f"{self.destination_prefix}{session_id}_events.zarr"
        
        # Directly save to S3 using BaseTransform's method
        self.save_dataset_to_s3_zarr(dataset, zarr_key)
        
        # Create metadata
        metadata = {
            "session_id": session_id,
            "processed_at": time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()),
            "storage_format": "zarr_xarray",
            # Add additional statistics as needed
            "task_count": len(processed_data['tasks']),
            "element_count": len(processed_data['elements']),
            "segment_count": sum(len(segments) for segments in processed_data['segments'].values())
        }
        
        return {
            "status": "success",
            "metadata": metadata,
            "files_to_copy": [],
            "files_to_upload": [],
            "zarr_stores": [zarr_key]
        }
    
    except Exception as e:
        # ... [existing error handling] ...
```

#### B. New Dataset Creation Method

```python
def _create_xarray_dataset(self, processed_data):
    """Convert processed event data into a single consolidated xarray Dataset.
    
    Args:
        processed_data: Dictionary of processed tasks, elements, and segments
        
    Returns:
        xarray Dataset containing all event data
    """
    import xarray as xr
    import numpy as np
    from collections import defaultdict
    
    # Extract component data
    tasks = processed_data['tasks']
    elements = processed_data['elements']
    segments = processed_data['segments']
    session_id = processed_data['session_id']
    
    # ------------ TASK DATA ------------
    task_ids = list(tasks.keys())
    data_vars = {}
    coords = {}
    
    if task_ids:
        # Create arrays for each task attribute
        task_arrays = defaultdict(list)
        
        # Define which task attributes to store
        task_attributes = [
            'task_type', 'start_time', 'end_time', 'duration', 
            'completion_status', 'session_fraction_start', 'session_fraction_end',
            'count', 'input_modality', 'audio_mode', 'skipped'
        ]
        
        # Fill arrays with task data
        for task_id in task_ids:
            task = tasks[task_id]
            for attr in task_attributes:
                # Use empty/default values if attribute doesn't exist
                task_arrays[attr].append(task.get(attr, None))
        
        # Add task data to dataset variables
        for attr, values in task_arrays.items():
            # Convert strings to numpy array with object dtype
            if all(isinstance(v, str) or v is None for v in values):
                values = np.array(values, dtype=object)
            else:
                values = np.array(values)
                
            data_vars[f'task_{attr}'] = ('task_id', values)
        
        # Add task_id as a coordinate
        coords['task_id'] = task_ids
    
    # ------------ ELEMENT DATA ------------
    element_ids = list(elements.keys())
    
    if element_ids:
        # Create arrays for each element attribute
        element_arrays = defaultdict(list)
        
        # Define which element attributes to store
        element_attributes = [
            'element_type', 'start_time', 'end_time', 'duration',
            'title', 'is_instruction', 'task_id', 'sequence_idx',
            'input_modality', 'audio_mode'
        ]
        
        # Fill arrays with element data
        for element_id in element_ids:
            element = elements[element_id]
            for attr in element_attributes:
                element_arrays[attr].append(element.get(attr, None))
        
        # Add element data to dataset variables
        for attr, values in element_arrays.items():
            # Convert strings to numpy array with object dtype
            if all(isinstance(v, str) or v is None for v in values):
                values = np.array(values, dtype=object)
            else:
                values = np.array(values)
                
            data_vars[f'element_{attr}'] = ('element_id', values)
        
        # Add element_id as a coordinate
        coords['element_id'] = element_ids
    
    # ------------ SEGMENT DATA ------------
    segment_types = list(segments.keys())
    
    if segment_types:
        all_segment_ids = []
        segment_type_indices = []
        segment_arrays = defaultdict(list)
        
        # Define which segment attributes to store
        segment_attributes = [
            'segment_type', 'start_time', 'end_time', 'duration',
            'containing_element_id', 'element_relative_start'
        ]
        
        # First pass - collect all unique segment ids
        for segment_type, segment_list in segments.items():
            for segment in segment_list:
                segment_id = segment['segment_id']
                all_segment_ids.append(segment_id)
                segment_type_indices.append(segment_type)
                
                for attr in segment_attributes:
                    segment_arrays[attr].append(segment.get(attr, None))
        
        # Add segment data to dataset
        for attr, values in segment_arrays.items():
            # Convert strings to numpy array with object dtype
            if all(isinstance(v, str) or v is None for v in values):
                values = np.array(values, dtype=object)
            else:
                values = np.array(values)
                
            data_vars[f'segment_{attr}'] = ('segment_id', values)
        
        # Add segment coordinates
        coords['segment_id'] = all_segment_ids
        data_vars['segment_type_index'] = ('segment_id', segment_type_indices)
    
    # ------------ RELATIONS DATA ------------
    # Add task-element relations
    if task_ids and element_ids:
        # Create a mapping from task_id to element_ids
        task_elements = defaultdict(list)
        for element_id, element in elements.items():
            task_id = element.get('task_id', '')
            if task_id:
                task_elements[task_id].append(element_id)
        
        # Add element counts per task
        element_counts = [len(task_elements.get(task_id, [])) for task_id in task_ids]
        data_vars['task_element_count'] = ('task_id', np.array(element_counts))
    
    # Create the dataset with all data variables and coordinates
    ds = xr.Dataset(
        data_vars=data_vars,
        coords=coords,
        attrs={
            'session_id': session_id,
            'transform': 'event',
            'version': '0.1',
            'created_at': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())
        }
    )
    
    # Apply chunking strategy
    chunk_sizes = {}
    if 'task_id' in coords and len(coords['task_id']) > 0:
        chunk_sizes['task_id'] = min(100, len(coords['task_id']))
    if 'element_id' in coords and len(coords['element_id']) > 0:
        chunk_sizes['element_id'] = min(100, len(coords['element_id']))
    if 'segment_id' in coords and len(coords['segment_id']) > 0:
        chunk_sizes['segment_id'] = min(100, len(coords['segment_id']))
    
    if chunk_sizes:
        ds = ds.chunk(chunk_sizes)
    
    return ds
```

### 3. Implementation Details

#### A. Removed Functions

The following function will be completely removed:

```python
def _save_processed_data(self, processed_data, dest_file):
    """This function is no longer needed and will be removed."""
    pass
```

#### B. Type Conversion Details

| HDF5 Type                          | Xarray/Numpy Type                |
|------------------------------------|----------------------------------|
| h5py.special_dtype(vlen=str)       | numpy.array(..., dtype=object)   |
| np.int32                           | numpy.array(..., dtype=np.int32) |
| np.float64                         | numpy.array(..., dtype=np.float64)|
| np.bool_                           | numpy.array(..., dtype=np.bool_) |

#### C. Memory Efficiency Considerations

1. Processing collections of data in batches
2. Using appropriate chunking for zarr storage
3. Leveraging xarray's lazy evaluation capabilities

## Benefits of this Approach

1. **Simplified Architecture**: One consolidated zarr store instead of multiple separate HDF5 groups
2. **Reduced Code**: Eliminates the complex HDF5 structure creation and saves directly to zarr
3. **Natural Querying**: xarray's dimension-based model provides natural, intuitive access patterns
4. **Better Performance**: Zarr format provides better parallel access and cloud storage efficiency
5. **Direct S3 Integration**: Uses BaseTransform's direct-to-S3 saving, eliminating local file steps
6. **Future Compatibility**: Aligns with the modern approach used in WindowTransform

## Testing and Verification Strategy

1. **Functional Validation**: Ensure all event data in existing HDF5 is correctly saved to zarr
2. **Performance Testing**: Compare memory usage and processing time with original implementation
3. **Data Retrieval**: Validate that all event relationships can be effectively queried from zarr
4. **Compatibility**: Ensure the rest of the pipeline can access the new zarr-based data format

## Conclusion

This simplified approach converts the EventTransform from HDF5 to zarr with minimal code changes while fully leveraging xarray and BaseTransform infrastructure. By creating a single consolidated zarr store with a more natural dimensional structure, we achieve better alignment with modern data practices and greater efficiency in storage and access.

The key benefits are:
- Elimination of unnecessary code and complexity
- Perfect alignment with BaseTransform infrastructure 
- Unified storage approach consistent with WindowTransform
- More efficient, cloud-optimized storage format