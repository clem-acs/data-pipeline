# Proposal: Zarr Append Functionality for BaseTransform

## Overview
This proposal outlines adding a method to BaseTransform that supports appending to existing Zarr stores in S3, especially for very large datasets where re-writing the entire store would be impractical.

## Implementation

### 1. New Method in BaseTransform

Add the following method to BaseTransform class (around line 960, after open_dataset_from_s3_zarr):

```python
def append_dataset_to_s3_zarr(self, new_dataset, store_key, append_dim='session', chunks=None):
    """
    Append new data to an existing Zarr store in S3 along a specified dimension.
    Optimized for very large datasets using Dask for lazy operations.
    
    Args:
        new_dataset: xarray Dataset with new data to append
        store_key: S3 key of the existing Zarr store
        append_dim: Dimension along which to append (default: 'session')
        chunks: Optional chunking configuration (if None, preserves existing chunks)
    
    Returns:
        S3 URI to the updated Zarr store
    """
    try:
        import xarray as xr
        import s3fs
    except ImportError:
        self.logger.info("Installing required packages...")
        import subprocess
        subprocess.check_call([sys.executable, "-m", "pip", "install", "xarray s3fs"])
        import xarray as xr
        import s3fs
    
    # Create S3 URI
    s3_uri = self.create_s3_zarr_store(store_key)
    self.logger.info(f"Checking for existing Zarr store at {s3_uri}")
    
    # If in dry run mode, just return
    if self.dry_run:
        self.logger.info(f"[DRY RUN] Would append to Zarr store at {s3_uri}")
        return s3_uri
    
    # Set up S3 filesystem
    s3 = s3fs.S3FileSystem(anon=False)
    store_path = s3_uri.replace('s3://', '')
    
    # Check if store exists
    store_exists = s3.exists(store_path)
    
    if not store_exists:
        self.logger.info(f"No existing Zarr store found at {s3_uri}, creating new store")
        return self.save_dataset_to_s3_zarr(new_dataset, store_key, chunks)
    
    # Create S3 mapper for Zarr
    s3_mapper = s3fs.S3Map(root=store_path, s3=s3, check=False)
    
    try:
        # Open existing dataset with Dask chunks to avoid loading all data
        existing_ds = xr.open_zarr(s3_mapper, consolidated=True, chunks='auto')
        
        # Check if the append dimension exists in both datasets
        if append_dim not in existing_ds.dims:
            self.logger.info(f"Creating new dimension '{append_dim}' in existing dataset")
            # If dimension doesn't exist, we need to add it
            existing_ds = existing_ds.expand_dims({append_dim: ['existing']})
        
        if append_dim not in new_dataset.dims:
            self.logger.info(f"Creating new dimension '{append_dim}' in new dataset")
            new_dataset = new_dataset.expand_dims({append_dim: ['new']})
        
        # Apply chunking to new dataset to match existing dataset if needed
        if not chunks and hasattr(existing_ds, 'chunks') and existing_ds.chunks:
            # Extract chunk sizes from existing dataset
            chunk_sizes = {}
            for dim, size in existing_ds.chunks.items():
                if size and len(size) > 0:
                    chunk_sizes[dim] = size[0]
            
            if chunk_sizes:
                self.logger.info(f"Using existing chunk sizes: {chunk_sizes}")
                new_dataset = new_dataset.chunk(chunk_sizes)
        
        # Prepare dataset for concatenation by ensuring compatible dimensions
        for dim in existing_ds.dims:
            if dim != append_dim and dim in new_dataset.dims:
                if existing_ds[dim].size != new_dataset[dim].size:
                    self.logger.warning(f"Dimension size mismatch for '{dim}': "
                                     f"existing={existing_ds[dim].size}, new={new_dataset[dim].size}")
        
        # Get the session IDs if appending along session dimension
        if append_dim == 'session':
            session_id = new_dataset.attrs.get('session_id', 'unknown_session')
            if isinstance(session_id, list):
                session_id = session_id[0] if session_id else 'unknown_session'
                
            # Create a session coordinate with just this session's ID
            new_dataset = new_dataset.assign_coords({append_dim: [session_id]})
        
        # Concatenate datasets using Dask (lazy operation)
        self.logger.info(f"Concatenating datasets along '{append_dim}' dimension")
        combined_ds = xr.concat([existing_ds, new_dataset], dim=append_dim)
        
        # Apply custom chunking if specified
        if chunks:
            self.logger.info(f"Applying custom chunking: {chunks}")
            combined_ds = combined_ds.chunk(chunks)
        
        # Write back to S3 (overwrites existing store)
        self.logger.info(f"Writing combined dataset to S3: {s3_uri}")
        combined_ds.to_zarr(s3_mapper, mode='w', consolidated=True)
        
        self.logger.info(f"Successfully appended to dataset at {s3_uri}")
        return s3_uri
        
    except Exception as e:
        self.logger.error(f"Error appending to zarr: {e}", exc_info=True)
        # Fall back to just saving the new dataset if append fails
        self.logger.warning(f"Append failed, falling back to saving only new data")
        # Create a unique timestamped backup key to avoid data loss
        backup_key = f"{store_key}.backup.{time.strftime('%Y%m%d_%H%M%S')}"
        self.logger.info(f"Creating backup of existing store at {backup_key}")
        
        # If possible, copy the existing store to a backup
        try:
            if not self.dry_run and store_exists:
                s3.copy(store_path, backup_key.replace('s3://', ''), recursive=True)
        except Exception as backup_e:
            self.logger.error(f"Failed to create backup: {backup_e}")
            
        # Save only the new dataset
        return self.save_dataset_to_s3_zarr(new_dataset, store_key, chunks)
```

### 2. Usage in Transforms

To use this in a transform, you would modify the process_session method like this:

```python
def process_session(self, session: Session) -> Dict:
    # ... existing processing code ...
    
    # Create xarray dataset as before
    ds = xr.Dataset(
        data_vars={
            'eeg': (eeg_dims, eeg_data),
            'fnirs': (fnirs_dims, fnirs_data),
            # ... other variables ...
        },
        coords={
            'time': timestamps,
        },
        attrs={
            'session_id': session_id,
            # ... other attributes ...
        }
    )
    
    # Set chunking as before
    chunks = {'time': min(100, n_windows)}
    ds = ds.chunk(chunks)
    
    # Define the destination Zarr store key - use a common key for appending
    # For example, store by modality instead of by session
    zarr_key = f"{self.destination_prefix}windows_combined.zarr"
    
    # Append to the existing store instead of creating a new one
    # The method will create a new store if none exists
    self.append_dataset_to_s3_zarr(ds, zarr_key, append_dim='session')
    
    # Return result as before
    return {
        "status": "success",
        "metadata": result_metadata,
        "files_to_copy": [],
        "files_to_upload": [],
        "zarr_stores": [zarr_key]
    }
```

### 3. Modifications for WindowTransform

For existing transforms like WindowTransform, we need to:

1. Add a session dimension to the dataset:
```python
# Modify line 293 to add session dimension
ds = xr.Dataset(
    data_vars={
        'eeg': (['session', 'time', 'eeg_channel', 'eeg_sample'], eeg_data.reshape(1, n_windows, *eeg_shape)),
        'fnirs': (['session', 'time', 'fnirs_channel'], fnirs_data.reshape(1, n_windows, *fnirs_shape)),
        # ... other variables with session dimension added ...
    },
    coords={
        'time': timestamps,
        'session': [session_id],  # Add session coordinate
    },
    attrs={
        'session_id': session_id,
        # ... other attributes ...
    }
)
```

2. Add an option to append to a common store:
```python
# Add a command-line argument
@classmethod
def add_subclass_arguments(cls, parser):
    # ... existing arguments ...
    parser.add_argument('--append-to-common', action='store_true',
                      help='Append to a common Zarr store instead of creating session-specific stores')
    parser.add_argument('--common-store-name', type=str, default='windows_combined',
                      help='Name for the common Zarr store when using --append-to-common')
```

3. Modify the storage logic to support both modes:
```python
# Using the argument in process_session
if args.append_to_common:
    zarr_key = f"{self.destination_prefix}{args.common_store_name}.zarr"
    self.append_dataset_to_s3_zarr(ds, zarr_key, append_dim='session')
else:
    # Original session-specific storage
    zarr_key = f"{self.destination_prefix}{session_id}_windowed.zarr"
    self.save_dataset_to_s3_zarr(ds, zarr_key)
```

## Handling Very Large Datasets

For extremely large datasets, additional considerations:

1. **Performance Optimization**:
   - Use aggressive chunking along session dimension
   - Consider implementing a worker pool for parallel processing

2. **Partitioning Strategy**:
   - For datasets exceeding practical limits, implement automatic partitioning
   - Create separate stores by time period (e.g., monthly/yearly)
   - Maintain a catalog file mapping sessions to their partition

3. **Monitoring and Recovery**:
   - Add progress tracking for long-running append operations
   - Implement checkpointing to allow resuming failed operations

## Implementation Timeline

1. Add the basic append_dataset_to_s3_zarr method (1 day)
2. Modify WindowTransform to support appending (1 day) 
3. Test with small and medium datasets (1 day)
4. Add partitioning for very large datasets (2-3 days)
5. Documentation (1/2 day)

Total: ~5-7 days for complete implementation